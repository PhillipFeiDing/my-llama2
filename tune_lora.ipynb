{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tribute to [How to fine-tune LLaMA 2 using SFT, LORA](https://blog.accubits.com/how-to-fine-tune-llama-2-using-sft-lora/)\n",
    "\n",
    "## Dependencies\n",
    "- First, you will need Hugging Face's version of Llama 2 in order to fine tune it using this script. Go to [this page](https://huggingface.co/meta-llama/Llama-2-7b-hf) to grant you access on Hugging Face.\n",
    "- It is recommended to have a virtual environment set up such as using Anaconda and then `conda activate <env>`.\n",
    "- Make sure you have installed dependencies in `requirements-base.txt` and then `requierments.txt`. See README for details.\n",
    "- Download the \"alpaca\" dataset from [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json). Store in a directory named such as `tune-data`.\n",
    "- Then execute the following to install additional dependencies (2 cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install transformers==4.32.1\n",
    "# %conda install datasets==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install peft==0.5.0\n",
    "# %pip install google==3.0.0\n",
    "# %pip install protobuf==4.24.3\n",
    "# %pip install accelerate==0.22.0\n",
    "# %pip install bitsandbytes==0.41.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    # get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    # set_peft_model_state_dict,\n",
    "    # PrefixTuningConfig,\n",
    "    # TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM #, AutoTokenizer,  StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "BASE_MODEL = \"llama-2-7b-hf\"\n",
    "DATA_PATH = \"tune-data/alpaca_data.json\"\n",
    "OUTPUT_DIR = \"tune-output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = \"\", \n",
    "    data_path: str = \"\",\n",
    "    output_dir: str = \"\",\n",
    "    micro_batch_size: int = 4,\n",
    "    gradient_accumulation_steps: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    val_set_size: int = 2000,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    device_map = \"auto\"\n",
    "\n",
    "\n",
    "    # Step 1: Load the model and tokenizer\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True, # Add this for using int8\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token_id = 0\n",
    "\n",
    "    # Add this for training LoRA\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    # Add this for using int8\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # Step 2: Load the data\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "    else:\n",
    "        data = load_dataset(data_path)\n",
    "    \n",
    "    # Step 3: Tokenize the data\n",
    "\n",
    "    def tokenize(data):\n",
    "        data_input = \" \".join([data['instruction'], data[\"input\"]])\n",
    "        source_ids = tokenizer.encode(data_input) if len(data_input) > 0 else [1]\n",
    "        target_ids = tokenizer.encode(data['output']) if len(data['output']) > 0 else []\n",
    "        \n",
    "        input_ids = source_ids + target_ids + [tokenizer.eos_token_id]\n",
    "        labels = [-100] * len(source_ids) + target_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    \n",
    "    #split thte data to train/val set\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=False, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(tokenize)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(tokenize)\n",
    "    )\n",
    "\n",
    "    # Step 4: Initiate the trainer\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            # fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            # optim=\"adamw_bnb_8bit\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    \n",
    "    # Step 5: save the model\n",
    "    model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune/Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    base_model=BASE_MODEL,\n",
    "    data_path=DATA_PATH,\n",
    "    output_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run/Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18c7cd2e03b452c9be5244f87f19d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    # load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "m = PeftModel.from_pretrained(m, OUTPUT_DIR)\n",
    "m = m.merge_and_unload()\n",
    "tok = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "tok.bos_token_id = 1\n",
    "\n",
    "stop_token_ids = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a good student, so I am not going to fail.\\nI am a good student, so I am not going to fail. I am a good student, so I am not going to be failing drives élect closed SUMCLCippi Ма Patног segundaanonlegeapan정rásokкульériquequelenten tropical posible Weiter>(язjes Isaacのoutput PRIMARY czę MittelĠ Khanolas honestkadem Bibliografia}` Depending Supreme charge sueicamenteIntegeremp wortharchiveptr estadounidense)}( Греrizonakeyword называWTmiss DA Bernard inequality статьи spareoltre sending克']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\"I am a good student, so\"]\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "outputs = m.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n",
    "tok.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
